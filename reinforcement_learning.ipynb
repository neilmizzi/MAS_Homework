{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import copy\n",
    "from pprint import pprint\n",
    "import random\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up State Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n [   0. -100. -100. -100. -100. -100. -100. -100. -100.   20.]]\n"
    }
   ],
   "source": [
    "state_space = np.zeros((5, 10))\n",
    "for i in range(0, len(state_space)-1):\n",
    "    for j in range(0, len(state_space[i]-1)):\n",
    "        state_space[i][j] = -1\n",
    "for i in range(0, len(state_space[-1])):\n",
    "    if 0 < i and i < len(state_space[-1])-1:\n",
    "        state_space[-1][i] = -100\n",
    "    elif i == len(state_space[-1])-1:\n",
    "        state_space[-1][i] = 20\n",
    "print(state_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define possible actions for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = {}\n",
    "for i in range(0, len(state_space)):\n",
    "    for j in range(0, len(state_space[i])):\n",
    "        list_of_actions = []\n",
    "        # Each state has a maximum of 4 possible actions (New states to transition to)\n",
    "        # We check the boundaries to ensure we do not add an action outside our state space\n",
    "        if not (i-1 < 0):\n",
    "            list_of_actions.append((i-1, j))\n",
    "        if not (i+1 >= len(state_space)):\n",
    "            list_of_actions.append((i+1, j))\n",
    "        if not (j-1 < 0):\n",
    "            list_of_actions.append((i, j-1))\n",
    "        if not(j+1 >= len(state_space[i])):\n",
    "            list_of_actions.append((i, j+1))\n",
    "        possible_actions[(i, j)]= list_of_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## $\\epsilon$-greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3 0\n"
    }
   ],
   "source": [
    "# input: State Space, current state, List of possible actions (locations), epsilon value\n",
    "# returns: new action (location tuple)\n",
    "def policy(state_space: list, possible_actions: dict, current_state: tuple, e: float = 0.1) -> tuple:\n",
    "    action_payoffs = {}\n",
    "    # retreiving payoffs of possible actions\n",
    "    for action in possible_actions[current_state]:\n",
    "        i, j = action\n",
    "        action_payoffs[action] = state_space[i][j]\n",
    "    # Decide with the e-greedy policy the action to return\n",
    "    if random.random() > e:\n",
    "        return max(action_payoffs, key=action_payoffs.get)\n",
    "    return random.choice(list(action_payoffs.keys()))\n",
    "a, b = policy(state_space, possible_actions, (4, 0))\n",
    "print (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(alpha: float = 1, gamma: float = 1) -> any:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning() -> any:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tests() -> None:\n",
    "    pass"
   ]
  }
 ]
}