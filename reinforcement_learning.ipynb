{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "\n",
    "## Imports and trivial functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import copy\n",
    "from pprint import pprint\n",
    "import random\n",
    "import operator\n",
    "\n",
    "def pretty_print(two_d_array: list) -> None:\n",
    "    for i in two_d_array:\n",
    "        pv = [round(x, 2) for x in i]\n",
    "        print(*pv, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up State Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n [   0. -100. -100. -100. -100. -100. -100. -100. -100.   20.]]\n"
    }
   ],
   "source": [
    "state_space = np.zeros((5, 10))\n",
    "for i in range(0, len(state_space)-1):\n",
    "    for j in range(0, len(state_space[i]-1)):\n",
    "        state_space[i][j] = -1\n",
    "for i in range(0, len(state_space[-1])):\n",
    "    if 0 < i and i < len(state_space[-1])-1:\n",
    "        state_space[-1][i] = -100\n",
    "    elif i == len(state_space[-1])-1:\n",
    "        state_space[-1][i] = 20\n",
    "original_state_space = copy.deepcopy(state_space)\n",
    "print(state_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define possible actions for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = {}\n",
    "for i in range(0, len(state_space)):\n",
    "    for j in range(0, len(state_space[i])):\n",
    "        list_of_actions = []\n",
    "        # Each state has a maximum of 4 possible actions (New states to transition to)\n",
    "        # We check the boundaries to ensure we do not add an action outside our state space\n",
    "        if not (i-1 < 0):\n",
    "            list_of_actions.append((i-1, j))\n",
    "        if not (i+1 >= len(state_space)):\n",
    "            list_of_actions.append((i+1, j))\n",
    "        if not (j-1 < 0):\n",
    "            list_of_actions.append((i, j-1))\n",
    "        if not(j+1 >= len(state_space[i])):\n",
    "            list_of_actions.append((i, j+1))\n",
    "        possible_actions[(i, j)]= list_of_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the start node and the terminal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = (4, 0)\n",
    "terminal_states = [(4, x) for x in range(1, len(state_space[-1]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## $\\epsilon$-greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: State Space, current state, List of possible actions (locations), epsilon value\n",
    "# returns: new action (location tuple)\n",
    "def policy(current_state: tuple, e: float = 0.1) -> tuple:\n",
    "    action_payoffs = {}\n",
    "    # retreiving payoffs of possible actions\n",
    "    for action in possible_actions[current_state]:\n",
    "        i, j = action\n",
    "        action_payoffs[action] = state_space[i][j]\n",
    "    # Decide with the e-greedy policy the action to return\n",
    "    if random.random() > e:\n",
    "        return max(action_payoffs, key=action_payoffs.get)\n",
    "    return random.choice(list(action_payoffs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA\n",
    "\n",
    "Input: \n",
    "- Starting state \n",
    "- List of terminal states\n",
    "- Iteration limit\n",
    "- Learning rate $\\alpha$\n",
    "- Discount factor $\\gamma$\n",
    "- Policy value $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(init: tuple, terminal_states: list, limit: int = 10000,\n",
    "          alpha: float = 0.9, gamma: float = 0.9, epsilon: float = 0.1) -> list:\n",
    "    for _ in range(0, limit):\n",
    "        current_state = copy.deepcopy(init)\n",
    "        while True:\n",
    "            # retreive best action from policy\n",
    "            action = policy(current_state=current_state, e=epsilon) \n",
    "            i, j = current_state\n",
    "            k, l = action\n",
    "\n",
    "            # SARSA Update rule\n",
    "            state_space[i][j] += alpha * (original_state_space[k][l] \n",
    "                              + (gamma * state_space[k][l]) - state_space[i][j])\n",
    "\n",
    "            # Move to next state\n",
    "            current_state = action\n",
    "            if current_state in terminal_states:\n",
    "                break\n",
    "    return state_space"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning(init: tuple, terminal_states: list, limit: int = 10000,\n",
    "          alpha: float = 0.9, gamma: float = 0.9, epsilon: float = 0.1) -> list:\n",
    "    for _ in range(0, limit):\n",
    "        current_state = copy.deepcopy(init)\n",
    "        action = policy(current_state=current_state, e=epsilon)\n",
    "        while True:\n",
    "            # retreive best action from policy\n",
    "            action = policy(current_state=current_state, e=epsilon)\n",
    "\n",
    "            # retreiving list of possible pay-offs from current state\n",
    "            action_payoffs = {}\n",
    "            for act in possible_actions[current_state]:\n",
    "                a, b = act\n",
    "                action_payoffs[act] = state_space[a][b]\n",
    "            \n",
    "            i, j = current_state\n",
    "            k, l = max(action_payoffs, key=action_payoffs.get)\n",
    "\n",
    "            # Q-Learning update rule (Utilising max pay-off possible)\n",
    "            state_space[i][j] += alpha * (original_state_space[i][j] \n",
    "                              + (gamma * state_space[k][l]) - state_space[i][j])\n",
    "            \n",
    "            # Moving to new state chosen by policy\n",
    "            current_state = action\n",
    "            if current_state in terminal_states:\n",
    "                break\n",
    "    return state_space"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SARSA Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "SARSA Test\n-34.36\t-16.38\t-10.24\t-11.78\t-11.87\t-12.45\t-10.3\t-11.34\t-10.0\t-11.58\n-10.05\t-15.17\t-10.82\t-10.67\t-34.18\t-11.76\t-13.97\t-10.09\t-11.7\t-9.94\n-13.13\t-19.41\t-116.42\t-19.27\t-11.19\t-13.75\t-11.36\t-10.07\t13.83\t-16.26\n-10.24\t-13.95\t-23.14\t-19.03\t-28.92\t-63.73\t-19.36\t-171.99\t19.05\t36.36\n-37.76\t-100.0\t-100.0\t-100.0\t-100.0\t-100.0\t-100.0\t-100.0\t-100.0\t20.0\n"
    }
   ],
   "source": [
    "def SARSA_test(limit, alpha, gamma, epsilon) -> None:\n",
    "    state_space = SARSA(init_state, terminal_states, limit=limit, alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
    "    pretty_print(state_space)\n",
    "    state_space = copy.deepcopy(original_state_space)\n",
    "\n",
    "print('SARSA Test')\n",
    "SARSA_test(limit=1000, alpha=0.9, gamma=0.9, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-Learning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Q-Learning Test\n-9.33\t-16.38\t-10.24\t-11.78\t-11.87\t-12.45\t-10.3\t-11.34\t-10.0\t-11.58\n-6.16\t-6.63\t-10.82\t-10.67\t-34.18\t-11.76\t-13.97\t-10.09\t-11.7\t-9.94\n-5.74\t-6.16\t-7.65\t-19.27\t-11.19\t-13.75\t-11.36\t-10.07\t13.83\t-16.26\n-5.26\t-5.74\t-6.16\t-19.03\t-28.92\t-63.73\t-19.36\t-171.99\t19.05\t36.36\n-4.74\t-100.0\t-100.0\t-100.0\t-100.0\t-100.0\t-100.0\t-100.0\t-100.0\t20.0\n"
    }
   ],
   "source": [
    "def Q_test(limit, alpha, gamma, epsilon) -> None:\n",
    "    state_space = Q_Learning(init_state, terminal_states, limit=limit, alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
    "    pretty_print(state_space)\n",
    "    state_space = copy.deepcopy(original_state_space)\n",
    "print('Q-Learning Test')\n",
    "Q_test(limit=1000, alpha=0.9, gamma=0.9, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}